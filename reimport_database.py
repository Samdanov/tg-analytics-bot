#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ü–û–õ–ù–û–ô –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤.

–í–ê–ñ–ù–û: –≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –æ–±–Ω–æ–≤–ª—è–µ—Ç:
1. Category –≤ —Ç–∞–±–ª–∏—Ü–µ channels (PRIMARY TOPIC - –Ω–µ —É—á–∞—Å—Ç–≤—É–µ—Ç –≤ TF-IDF!)
2. Keywords –≤ —Ç–∞–±–ª–∏—Ü–µ keywords_cache (—Ç–æ–ª—å–∫–æ –∏–∑ title + description)

–ö–∞–Ω–∞–ª—ã, –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ LLM, –ù–ï –±—É–¥—É—Ç –∑–∞—Ç—Ä–æ–Ω—É—Ç—ã (—É –Ω–∏—Ö –µ—Å—Ç—å audience).

–ü–ï–†–ï–î –ó–ê–ü–£–°–ö–û–ú:
    python -m app.services.migrate_add_category  # –î–æ–±–∞–≤–∏—Ç—å –∫–æ–ª–æ–Ω–∫—É category

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python reimport_database.py [max_rows] [min_subscribers]

–ü—Ä–∏–º–µ—Ä—ã:
    python reimport_database.py                    # –í—Å–µ –∫–∞–Ω–∞–ª—ã
    python reimport_database.py 10000              # –ü–µ—Ä–≤—ã–µ 10000
    python reimport_database.py 100000 1000        # 100–ö —Å >1000 –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤
"""

import asyncio
import sys
import json
from pathlib import Path
from typing import Optional

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
ROOT = Path(__file__).resolve().parent
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

import pandas as pd
from sqlalchemy import select, update

from app.core.logging import setup_logging, get_logger
from app.db.database import async_session_maker
from app.db.models import Channel, KeywordsCache
from app.services.excel_importer import extract_keywords_v2  # –ß–∏—Å—Ç–∞—è –≤–µ—Ä—Å–∏—è –±–µ–∑ category –≤ keywords!

logger = get_logger(__name__)

EXCEL_PATH = "/home/alex/excel/DB_channel.xlsx"


async def update_database_from_excel(
    max_rows: Optional[int] = None,
    min_subscribers: int = 0,
    skip_llm_analyzed: bool = True
):
    """
    –ü–û–õ–ù–ê–Ø –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∞ –±–∞–∑—ã: –æ–±–Ω–æ–≤–ª—è–µ—Ç category + keywords –¥–ª—è –∫–∞–Ω–∞–ª–æ–≤.
    
    –í–ê–ñ–ù–û:
    - Category —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ Channel.category (PRIMARY TOPIC)
    - Keywords –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è –¢–û–õ–¨–ö–û –∏–∑ title + description (–±–µ–∑ category!)
    - Category –ù–ï —É—á–∞—Å—Ç–≤—É–µ—Ç –≤ TF-IDF similarity
    
    Args:
        max_rows: –ú–∞–∫—Å–∏–º—É–º —Å—Ç—Ä–æ–∫
        min_subscribers: –ú–∏–Ω–∏–º—É–º –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤
        skip_llm_analyzed: –ü—Ä–æ–ø—É—Å–∫–∞—Ç—å –∫–∞–Ω–∞–ª—ã —Å LLM –∞–Ω–∞–ª–∏–∑–æ–º (audience –Ω–µ –ø—É—Å—Ç–æ–π)
    """
    logger.info("="*60)
    logger.info("–ü–û–õ–ù–ê–Ø –ü–ï–†–ï–°–ë–û–†–ö–ê –ë–ê–ó–´ –ö–ê–ù–ê–õ–û–í")
    logger.info("="*60)
    logger.info("üìå Category ‚Üí Channel.category (PRIMARY TOPIC, –Ω–µ —É—á–∞—Å—Ç–≤—É–µ—Ç –≤ TF-IDF)")
    logger.info("üìå Keywords ‚Üí —Ç–æ–ª—å–∫–æ –∏–∑ title + description")
    logger.info("="*60)
    
    logger.info(f"üìÇ –ß–∏—Ç–∞—é Excel: {EXCEL_PATH}")
    df = pd.read_excel(EXCEL_PATH, header=1)
    
    if max_rows:
        df = df.iloc[:max_rows]
    
    logger.info(f"üìä –°—Ç—Ä–æ–∫ –≤ Excel: {len(df)}")
    
    # –°–æ–∑–¥–∞—ë–º –º–∞–ø–ø–∏–Ω–≥ username -> (category, title, description)
    excel_data = {}
    categories_found = set()
    
    for _, row in df.iterrows():
        username = str(row.get("username") or "").strip()
        username = username.replace("@", "").replace("https://t.me/", "").replace("http://t.me/", "")
        
        if not username:
            continue
            
        try:
            subscribers = int(row.get("subscribers") or 0)
        except Exception:
            subscribers = 0
            
        if subscribers < min_subscribers:
            continue
        
        title = str(row.get("title") or "").strip()
        description = str(row.get("description") or "").strip()
        category = str(row.get("category") or "").strip()
        
        if title.lower() in ("nan", "none"):
            title = ""
        if description.lower() in ("nan", "none"):
            description = ""
        if category.lower() in ("nan", "none"):
            category = ""
        
        if category:
            categories_found.add(category)
        
        excel_data[username.lower()] = {
            "title": title,
            "description": description,
            "category": category,
        }
    
    logger.info(f"üìã –ö–∞–Ω–∞–ª–æ–≤ –≤ Excel —Å –¥–∞–Ω–Ω—ã–º–∏: {len(excel_data)}")
    logger.info(f"üìã –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {len(categories_found)}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    updated_channels = 0
    updated_keywords = 0
    skipped_llm = 0
    skipped_not_found = 0
    
    async with async_session_maker() as session:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –∫–∞–Ω–∞–ª—ã —Å keywords
        q = select(
            Channel.id,
            Channel.username,
            KeywordsCache.audience
        ).join(KeywordsCache, KeywordsCache.channel_id == Channel.id)
        
        rows = (await session.execute(q)).all()
        logger.info(f"üìä –ö–∞–Ω–∞–ª–æ–≤ –≤ –ë–î —Å keywords: {len(rows)}")
        
        batch_size = 1000
        channel_batch = []
        keywords_batch = []
        
        for channel_id, username, audience in rows:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º LLM-–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ
            if skip_llm_analyzed and audience and len(audience) > 50:
                skipped_llm += 1
                continue
            
            # –ò—â–µ–º –≤ Excel
            excel_info = excel_data.get(username.lower() if username else "")
            if not excel_info:
                skipped_not_found += 1
                continue
            
            # 1. –û–±–Ω–æ–≤–ª—è–µ–º category –≤ Channel (PRIMARY TOPIC)
            channel_batch.append({
                "channel_id": channel_id,
                "category": excel_info["category"],
            })
            
            # 2. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ß–ò–°–¢–´–ï keywords (—Ç–æ–ª—å–∫–æ title + description!)
            new_keywords = extract_keywords_v2(
                excel_info["title"],
                excel_info["description"],
                limit=20
            )
            
            # Fallback: –µ—Å–ª–∏ keywords –ø—É—Å—Ç—ã–µ - –±–µ—Ä—ë–º username
            if not new_keywords:
                new_keywords = [username.lower()] if username else []
            
            keywords_batch.append({
                "channel_id": channel_id,
                "keywords_json": json.dumps(new_keywords, ensure_ascii=False)
            })
            
            # Batch commit
            if len(channel_batch) >= batch_size:
                # Update channels (category)
                for item in channel_batch:
                    await session.execute(
                        update(Channel)
                        .where(Channel.id == item["channel_id"])
                        .values(category=item["category"])
                    )
                
                # Update keywords_cache
                for item in keywords_batch:
                    await session.execute(
                        update(KeywordsCache)
                        .where(KeywordsCache.channel_id == item["channel_id"])
                        .values(keywords_json=item["keywords_json"])
                    )
                
                await session.commit()
                updated_channels += len(channel_batch)
                updated_keywords += len(keywords_batch)
                logger.info(f"‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–æ: {updated_channels} –∫–∞–Ω–∞–ª–æ–≤, {updated_keywords} keywords")
                channel_batch = []
                keywords_batch = []
        
        # –û—Å—Ç–∞—Ç–æ–∫
        if channel_batch:
            for item in channel_batch:
                await session.execute(
                    update(Channel)
                    .where(Channel.id == item["channel_id"])
                    .values(category=item["category"])
                )
            
            for item in keywords_batch:
                await session.execute(
                    update(KeywordsCache)
                    .where(KeywordsCache.channel_id == item["channel_id"])
                    .values(keywords_json=item["keywords_json"])
                )
            
            await session.commit()
            updated_channels += len(channel_batch)
            updated_keywords += len(keywords_batch)
    
    logger.info("="*60)
    logger.info("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    logger.info(f"   ‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–æ Channel.category: {updated_channels}")
    logger.info(f"   ‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–æ keywords (—á–∏—Å—Ç—ã–µ): {updated_keywords}")
    logger.info(f"   ‚è≠Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ (LLM): {skipped_llm}")
    logger.info(f"   ‚è≠Ô∏è  –ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤ Excel: {skipped_not_found}")
    logger.info("="*60)
    
    return updated_channels, updated_keywords


async def main():
    setup_logging()
    
    max_rows = int(sys.argv[1]) if len(sys.argv) >= 2 else None
    min_subs = int(sys.argv[2]) if len(sys.argv) >= 3 else 0
    
    logger.info(f"üöÄ –°—Ç–∞—Ä—Ç –ø–æ–ª–Ω–æ–π –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∏ –±–∞–∑—ã")
    if max_rows:
        logger.info(f"   –õ–∏–º–∏—Ç —Å—Ç—Ä–æ–∫: {max_rows}")
    if min_subs:
        logger.info(f"   –ú–∏–Ω–∏–º—É–º –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤: {min_subs}")
    
    updated_channels, updated_keywords = await update_database_from_excel(
        max_rows=max_rows,
        min_subscribers=min_subs,
        skip_llm_analyzed=True
    )
    
    logger.info("üéâ –ì–æ—Ç–æ–≤–æ!")
    logger.info("")
    logger.info("üìã –ß–¢–û –ò–ó–ú–ï–ù–ò–õ–û–°–¨:")
    logger.info("   1. Channel.category = PRIMARY TOPIC (48 —Ç–µ–º –∏–∑ Excel)")
    logger.info("   2. Keywords = –¢–û–õ–¨–ö–û title + description (–±–µ–∑ category!)")
    logger.info("   3. Category –ù–ï —É—á–∞—Å—Ç–≤—É–µ—Ç –≤ TF-IDF similarity")
    logger.info("")
    logger.info("üìã –°–õ–ï–î–£–Æ–©–ò–ô –®–ê–ì:")
    logger.info("   –ü–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å similarity —Å –Ω–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏:")
    logger.info("   python -m app.services.similarity_engine.cli seq 500")


if __name__ == "__main__":
    asyncio.run(main())

