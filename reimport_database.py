#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø–µ—Ä–µ–∏–º–ø–æ—Ä—Ç–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤ —Å –Ω–æ–≤—ã–º –∞–ª–≥–æ—Ä–∏—Ç–º–æ–º keywords.

–í–ù–ò–ú–ê–ù–ò–ï: –≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –æ–±–Ω–æ–≤–∏—Ç –í–°–ï keywords –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö!
–ö–∞–Ω–∞–ª—ã, –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ LLM, –ù–ï –±—É–¥—É—Ç –∑–∞—Ç—Ä–æ–Ω—É—Ç—ã (—É –Ω–∏—Ö –µ—Å—Ç—å audience).

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python reimport_database.py [max_rows] [min_subscribers]

–ü—Ä–∏–º–µ—Ä—ã:
    python reimport_database.py                    # –í—Å–µ –∫–∞–Ω–∞–ª—ã
    python reimport_database.py 10000              # –ü–µ—Ä–≤—ã–µ 10000
    python reimport_database.py 100000 1000        # 100–ö —Å >1000 –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤
"""

import asyncio
import sys
import json
import re
from pathlib import Path
from typing import Set, List, Optional

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
ROOT = Path(__file__).resolve().parent
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

import pandas as pd
from sqlalchemy import select, update, text

from app.core.logging import setup_logging, get_logger
from app.db.database import async_session_maker
from app.db.models import Channel, KeywordsCache
from app.services.excel_importer import extract_keywords_v2, CATEGORY_KEYWORDS

logger = get_logger(__name__)

EXCEL_PATH = "/home/alex/excel/DB_channel.xlsx"


async def update_keywords_from_excel(
    max_rows: Optional[int] = None,
    min_subscribers: int = 0,
    skip_llm_analyzed: bool = True
):
    """
    –û–±–Ω–æ–≤–ª—è–µ—Ç keywords –¥–ª—è –∫–∞–Ω–∞–ª–æ–≤ –∏–∑ Excel —Ñ–∞–π–ª–∞.
    
    Args:
        max_rows: –ú–∞–∫—Å–∏–º—É–º —Å—Ç—Ä–æ–∫
        min_subscribers: –ú–∏–Ω–∏–º—É–º –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤
        skip_llm_analyzed: –ü—Ä–æ–ø—É—Å–∫–∞—Ç—å –∫–∞–Ω–∞–ª—ã —Å LLM –∞–Ω–∞–ª–∏–∑–æ–º (audience –Ω–µ –ø—É—Å—Ç–æ–π)
    """
    logger.info("="*60)
    logger.info("–ü–ï–†–ï–ò–ú–ü–û–†–¢ KEYWORDS –° –ù–û–í–´–ú –ê–õ–ì–û–†–ò–¢–ú–û–ú")
    logger.info("="*60)
    
    logger.info(f"üìÇ –ß–∏—Ç–∞—é Excel: {EXCEL_PATH}")
    df = pd.read_excel(EXCEL_PATH, header=1)
    
    if max_rows:
        df = df.iloc[:max_rows]
    
    logger.info(f"üìä –°—Ç—Ä–æ–∫ –≤ Excel: {len(df)}")
    
    # –°–æ–∑–¥–∞—ë–º –º–∞–ø–ø–∏–Ω–≥ username -> (category, title, description)
    excel_data = {}
    for _, row in df.iterrows():
        username = str(row.get("username") or "").strip()
        username = username.replace("@", "").replace("https://t.me/", "").replace("http://t.me/", "")
        
        if not username:
            continue
            
        try:
            subscribers = int(row.get("subscribers") or 0)
        except:
            subscribers = 0
            
        if subscribers < min_subscribers:
            continue
        
        title = str(row.get("title") or "").strip()
        description = str(row.get("description") or "").strip()
        category = str(row.get("category") or "").strip()
        
        if title.lower() == "nan":
            title = ""
        if description.lower() == "nan":
            description = ""
        if category.lower() == "nan":
            category = ""
        
        excel_data[username.lower()] = {
            "title": title,
            "description": description,
            "category": category,
        }
    
    logger.info(f"üìã –ö–∞–Ω–∞–ª–æ–≤ –≤ Excel —Å –¥–∞–Ω–Ω—ã–º–∏: {len(excel_data)}")
    
    # –û–±–Ω–æ–≤–ª—è–µ–º keywords –≤ –ë–î
    updated = 0
    skipped_llm = 0
    skipped_not_found = 0
    
    async with async_session_maker() as session:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –∫–∞–Ω–∞–ª—ã —Å keywords
        q = select(
            Channel.id,
            Channel.username,
            KeywordsCache.audience
        ).join(KeywordsCache, KeywordsCache.channel_id == Channel.id)
        
        rows = (await session.execute(q)).all()
        logger.info(f"üìä –ö–∞–Ω–∞–ª–æ–≤ –≤ –ë–î —Å keywords: {len(rows)}")
        
        batch_size = 1000
        batch = []
        
        for channel_id, username, audience in rows:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º LLM-–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ
            if skip_llm_analyzed and audience and len(audience) > 50:
                skipped_llm += 1
                continue
            
            # –ò—â–µ–º –≤ Excel
            excel_info = excel_data.get(username.lower() if username else "")
            if not excel_info:
                skipped_not_found += 1
                continue
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ keywords
            new_keywords = extract_keywords_v2(
                excel_info["title"],
                excel_info["description"],
                excel_info["category"],
                limit=20
            )
            
            if not new_keywords:
                new_keywords = [excel_info["category"]] if excel_info["category"] else [username]
            
            batch.append({
                "channel_id": channel_id,
                "keywords_json": json.dumps(new_keywords, ensure_ascii=False)
            })
            
            if len(batch) >= batch_size:
                # Batch update
                for item in batch:
                    await session.execute(
                        update(KeywordsCache)
                        .where(KeywordsCache.channel_id == item["channel_id"])
                        .values(keywords_json=item["keywords_json"])
                    )
                await session.commit()
                updated += len(batch)
                logger.info(f"‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–æ: {updated}")
                batch = []
        
        # –û—Å—Ç–∞—Ç–æ–∫
        if batch:
            for item in batch:
                await session.execute(
                    update(KeywordsCache)
                    .where(KeywordsCache.channel_id == item["channel_id"])
                    .values(keywords_json=item["keywords_json"])
                )
            await session.commit()
            updated += len(batch)
    
    logger.info("="*60)
    logger.info("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    logger.info(f"   ‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–æ keywords: {updated}")
    logger.info(f"   ‚è≠Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ (LLM): {skipped_llm}")
    logger.info(f"   ‚è≠Ô∏è  –ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤ Excel: {skipped_not_found}")
    logger.info("="*60)
    
    return updated


async def main():
    setup_logging()
    
    max_rows = int(sys.argv[1]) if len(sys.argv) >= 2 else None
    min_subs = int(sys.argv[2]) if len(sys.argv) >= 3 else 0
    
    logger.info(f"üöÄ –°—Ç–∞—Ä—Ç –ø–µ—Ä–µ–∏–º–ø–æ—Ä—Ç–∞")
    if max_rows:
        logger.info(f"   –õ–∏–º–∏—Ç —Å—Ç—Ä–æ–∫: {max_rows}")
    if min_subs:
        logger.info(f"   –ú–∏–Ω–∏–º—É–º –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤: {min_subs}")
    
    await update_keywords_from_excel(
        max_rows=max_rows,
        min_subscribers=min_subs,
        skip_llm_analyzed=True
    )
    
    logger.info("üéâ –ì–æ—Ç–æ–≤–æ!")
    logger.info("")
    logger.info("üìã –°–õ–ï–î–£–Æ–©–ò–ô –®–ê–ì:")
    logger.info("   –ü–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å similarity:")
    logger.info("   python -m app.services.similarity_engine.cli seq 500")


if __name__ == "__main__":
    asyncio.run(main())

